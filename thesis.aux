\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\catcode `"\active 
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\vspace {-\cftbeforepartskip }}
\@writefile{lof}{\deactivateaddvspace  }
\@writefile{lot}{\deactivateaddvspace  }
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}\spacedlowsmallcaps  {Introduction}}{9}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:intro}{{1}{9}{\texorpdfstring {\spacedlowsmallcaps {Introduction}}{Introduction}}{chapter.4}{}}
\@writefile{toc}{\vskip 10pt}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{9}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The left is a still picture of a jokey riding a horse. One can easily tell that the horse is running. However, can you guess what is the horse gait? Is it walk, trot, canter or gallop? Only by considering more images, on the right, you can easily tell that the horse locomotion is definitely$^{\ddagger }$.\relax }}{9}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{ch1:fig:1-3}{{1}{9}{The left is a still picture of a jokey riding a horse. One can easily tell that the horse is running. However, can you guess what is the horse gait? Is it walk, trot, canter or gallop? Only by considering more images, on the right, you can easily tell that the horse locomotion is definitely$^{\ddagger }$.\relax }{figure.caption.6}{}}
\newlabel{ch1:sec:intro}{{1.1}{12}{Introduction}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Method}{12}{section.8}}
\newlabel{ch1:sec:model}{{1.2}{12}{Method}{section.8}{}}
\citation{searle1980minds}
\citation{takahashi2017aenet,kumar2016audio,jing2016discriminative}
\citation{jiang2014zero,wu2014zero,jiang2014easy}
\citation{mazloom2014conceptlets,mazloom2015tagbook}
\citation{hussein2017unified,habibian2014videostory,habibian2017video2vec}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Can you guess what is the human activity in this picture?$^{\dagger }$ A complex human activity is analogous to a mosaic painting. The activity is made of small pieces, called atomic actions. Each, by itself, does not tell the complete story of the activity. But only when all are put together, these pieces paint the clear picture of the complex activity.\relax }}{14}{figure.caption.10}}
\newlabel{ch1:fig:1-1}{{2}{14}{Can you guess what is the human activity in this picture?$^{\dagger }$ A complex human activity is analogous to a mosaic painting. The activity is made of small pieces, called atomic actions. Each, by itself, does not tell the complete story of the activity. But only when all are put together, these pieces paint the clear picture of the complex activity.\relax }{figure.caption.10}{}}
\newlabel{ch1:fig:1-2-1}{{3a}{14}{Wash Hands\relax }{figure.caption.11}{}}
\newlabel{sub@ch1:fig:1-2-1}{{a}{14}{Wash Hands\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Look only at the leftmost picture, it shows the atomic action of ``wash hands". Can you figure out the overarching goal of the rest of the video? Only after you consider the other atomic actions, you can tell.\relax }}{14}{figure.caption.11}}
\newlabel{ch1:fig:1-2}{{3}{14}{Look only at the leftmost picture, it shows the atomic action of ``wash hands". Can you figure out the overarching goal of the rest of the video? Only after you consider the other atomic actions, you can tell.\relax }{figure.caption.11}{}}
\citation{dalal2005histograms}
\citation{bay2006surf}
\citation{dalal2005histograms}
\citation{wang2011action}
\citation{wang2013action}
\citation{horn1981determining}
\citation{peng2016bag}
\citation{simonyan2013fisher}
\citation{arandjelovic2013all}
\citation{schuldt2004recognizing,tong2001support}
\citation{pal1992multilayer}
\citation{fleet2006optical}
\citation{bilen2016dynamic}
\citation{vieira2012stop}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Related Work}{18}{section.12}}
\newlabel{ch1:sec:related}{{1.3}{18}{Related Work}{section.12}{}}
\citation{fukushima1975cognitron,fukushima1980neocognitron,lecun1998gradient}
\citation{he2016deep,krizhevsky2012imagenet}
\citation{tran2015learning,ji20133d}
\citation{hara2017can}
\citation{carreira2017quo}
\citation{wang2018temporal}
\citation{tran2019video}
\citation{wang2018non}
\citation{li2018videolstm,li2017concurrent}
\citation{miech2017learnable}
\citation{lea2017temporal}
\citation{lea2017temporal}
\citation{wu2019long}
\citation{soomro2012ucf101}
\citation{karpathy2014large}
\citation{idrees2017thumos}
\citation{heilbron2015activitynet}
\citation{kuehne2014language}
\citation{Zhou2017YouCookIID}
\citation{damen2018scaling}
\citation{rohrbach2016recognizing}
\citation{sigurdsson2016hollywood}
\citation{kay2017kinetics}
\citation{abu2016youtube}
\citation{ye2015eventnet}
\citation{jiangfcvid}
\citation{thomee2016yfcc100m}
\citation{marszalek2009actions}
\citation{kuehne2011hmdb}
\citation{gu2017ava}
\citation{rohrbach2015dataset}
\citation{pini2019m}
\citation{hussein2017unified}
\citation{hussein2017unified}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}List of Publications}{19}{section.13}}
\newlabel{ch1:sec:publications}{{1.4}{19}{List of Publications}{section.13}{}}
\citation{hussein2019videograph}
\citation{hussein2017unified}
\citation{hussein2017unified}
\citation{over2013trecvid,over2014trecvid}
\citation{hussein2017unified}
\citation{jiang2015bridging,habibian2014composite,habibian2015discovering}
\citation{jiang2015bridging,mazloom2014conceptlets,chang2015semantic,chang2016dynamic,changthey,lu2016zero}
\citation{jiang2015bridging,chang2016dynamic,changthey,jiang2015fast}
\citation{mensink2014costa,gavves2015activetransferlearning}
\citation{ye2015eventnet}
\citation{wikihow}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}\spacedlowsmallcaps  {Unified Embedding and Metric Learning for Zero-Exemplar Event Detection}}{21}{chapter.14}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:unified-embedding}{{2}{21}{\texorpdfstring {\spacedlowsmallcaps {Unified Embedding and Metric Learning for Zero-Exemplar Event Detection}}{Unified Embedding and Metric Learning for Zero-Exemplar Event Detection}}{chapter.14}{}}
\@writefile{toc}{\vskip 10pt}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{21}{section.15}}
\citation{over2013trecvid}
\citation{over2014trecvid}
\citation{habibian2014videostory,habibian2015videostory}
\citation{mazloom2015tagbook}
\citation{chang2015semantic,chang2016dynamic,changthey,lu2016zero,jiang2015bridging}
\citation{changthey}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces We pose the problem of zero-exemplar event detection as learning from a repository of pre-defined events. Given video exemplars of events ``removing drywall" or ``fit wall times", one may detect a novel event ``renovate home" as a probability distribution over the predefined events.\relax }}{22}{figure.caption.17}}
\newlabel{ch2:fig:1-1}{{4}{22}{We pose the problem of zero-exemplar event detection as learning from a repository of pre-defined events. Given video exemplars of events ``removing drywall" or ``fit wall times", one may detect a novel event ``renovate home" as a probability distribution over the predefined events.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Related Work}{22}{section.21}}
\newlabel{ch2:sec:related}{{2.2}{22}{Related Work}{section.21}{}}
\citation{wu2014zero}
\citation{elhoseiny2015zero}
\citation{mikolov2013exploiting,mikolov2013distributed}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Three families of methods for zero-exemplar event detection: (a), (b) and (c). They build on top of feature representations learned a priori (i.e. initial representations), such as CNN features $x$ for a video $v$ or word2vec features $y$ for event text query $t$. In a post-processing step, the distance $\theta $ is measured between the embedded features. In contrast, our model rather falls in a new family, depicted in (d), for it learns unified embedding with metric loss using single data source.\relax }}{23}{figure.caption.22}}
\newlabel{ch2:fig:2-1}{{5}{23}{Three families of methods for zero-exemplar event detection: (a), (b) and (c). They build on top of feature representations learned a priori (i.e. initial representations), such as CNN features $x$ for a video $v$ or word2vec features $y$ for event text query $t$. In a post-processing step, the distance $\theta $ is measured between the embedded features. In contrast, our model rather falls in a new family, depicted in (d), for it learns unified embedding with metric loss using single data source.\relax }{figure.caption.22}{}}
\citation{wu2014zero,elhoseiny2015zero}
\citation{wu2014zero,elhoseiny2015zero}
\citation{jiang2014easy}
\citation{jiang2014zero}
\citation{agharwal2016tag}
\citation{tran2014c3d,simonyan2014two,wang2011action,uijlings2015video,fernandoTPAMIrankpooling}
\citation{muda2010voice,kumar2016audio,jing2016discriminative}
\citation{habibian2015videostory}
\citation{lecun1998gradient,he2015deep,krizhevsky2012imagenet,simonyan2014very}
\citation{gan2016you}
\citation{simonyan2013fisher,arandjelovic2013all}
\citation{lu2016zero,mettes2015bag}
\citation{habibian2014videostory,habibian2015videostory}
\citation{wang2011action,uijlings2015video}
\citation{tran2014c3d,simonyan2014two}
\citation{muda2010voice,kumar2016audio,jing2016discriminative}
\citation{sutskever2014sequence}
\citation{mikolov2013distributed,le2014distributed}
\citation{blei2003latent,deerwester1990indexing}
\citation{habibian2015videostory}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Model overview. Using dataset $\mathcal  {D}^{z}$ of $M$ event categories and $N$ videos. Each event has a text article and a few videos. Given a video $x$ with text title $k$, belonging to an event with article $t$, we \textbf  {extract} features $x, y^k, y^t$ respectively. At the top, network $f_\mathcal  {T}$ \textbf  {learns} to classify the title feature $y^k$ into one of $M$ event categories. In the middle, we borrow the network $f_\mathcal  {T}$ to \textbf  {embed} the event article's feature $y^t$ as $z^t \in \mathcal  {Z}$. Then, at the bottom, the network $f_\mathcal  {V}$ \textbf  {learns} to embed the video feature $x$ as $z^v \in \mathcal  {Z}$ such that the distance between $\left (z^v, z^t\right )$ is minimized, in the learned metric space $\mathcal  {Z}$.\relax }}{24}{figure.caption.23}}
\newlabel{ch2:fig:3-1}{{6}{24}{Model overview. Using dataset $\mathcal {D}^{z}$ of $M$ event categories and $N$ videos. Each event has a text article and a few videos. Given a video $x$ with text title $k$, belonging to an event with article $t$, we \textbf {extract} features $x, y^k, y^t$ respectively. At the top, network $f_\mathcal {T}$ \textbf {learns} to classify the title feature $y^k$ into one of $M$ event categories. In the middle, we borrow the network $f_\mathcal {T}$ to \textbf {embed} the event article's feature $y^t$ as $z^t \in \mathcal {Z}$. Then, at the bottom, the network $f_\mathcal {V}$ \textbf {learns} to embed the video feature $x$ as $z^v \in \mathcal {Z}$ such that the distance between $\left (z^v, z^t\right )$ is minimized, in the learned metric space $\mathcal {Z}$.\relax }{figure.caption.23}{}}
\newlabel{sub:auxiliary}{{2.2}{24}{Related Work}{figure.caption.23}{}}
\citation{chopra2005learning}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Method}{25}{section.24}}
\newlabel{ch2:sec:model}{{2.3}{25}{Method}{section.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Overview}{25}{subsection.25}}
\newlabel{ch2:eq:def}{{\relax 2.1}{25}{Overview}{equation.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Model}{25}{subsection.27}}
\citation{habibian2014videostory,habibian2015videostory}
\newlabel{ch2:eq:loss_contrastive_u}{{\relax 2.2}{26}{Model}{equation.28}{}}
\newlabel{ch2:eq:loss_distance_u}{{\relax 2.3}{26}{Model}{equation.29}{}}
\newlabel{ch2:eq:loss_logistic_u}{{\relax 2.4}{26}{Model}{equation.30}{}}
\citation{he2015deep}
\citation{deerwester1990indexing}
\citation{le2014distributed}
\citation{ye2015eventnet}
\citation{wikihow}
\newlabel{ch2:eq:loss_model_u}{{\relax 2.5}{27}{Model}{equation.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Experiments}{27}{section.32}}
\newlabel{ch2:sec:experiments}{{2.4}{27}{Experiments}{section.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Datasets}{27}{subsection.33}}
\citation{over2013trecvid}
\citation{over2013trecvid}
\citation{he2015deep}
\citation{simonyan2014very}
\citation{szegedy2015going}
\citation{zhou2016places}
\citation{blei2003latent,deerwester1990indexing}
\citation{deerwester1990indexing}
\citation{wikipedia}
\citation{blei2003latent}
\citation{kiros2015skip}
\citation{le2014distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Implementation Details}{28}{subsection.34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Textual Embedding}{28}{subsection.36}}
\newlabel{ch2:fig:4-2-1}{{\caption@xref {ch2:fig:4-2-1}{ on input line 33}}{29}{Textual Embedding}{figure.caption.37}{}}
\newlabel{ch2:fig:4-2-2}{{\caption@xref {ch2:fig:4-2-2}{ on input line 36}}{29}{Textual Embedding}{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Our textual embedding (b) maps MED to EventNet events better than LSI features. Each dot in the matrix shows the similarity between MED and EventNet events.\relax }}{29}{figure.caption.37}}
\newlabel{ch2:fig:4-2}{{7}{29}{Our textual embedding (b) maps MED to EventNet events better than LSI features. Each dot in the matrix shows the similarity between MED and EventNet events.\relax }{figure.caption.37}{}}
\newlabel{ch2:fig:4-1-1}{{\caption@xref {ch2:fig:4-1-1}{ on input line 46}}{29}{Textual Embedding}{figure.caption.38}{}}
\newlabel{ch2:fig:4-1-2}{{\caption@xref {ch2:fig:4-1-2}{ on input line 48}}{29}{Textual Embedding}{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces For 20 events of MED-14, our textual embedding (right) is more discriminant than the LSI feature representation (left). Each dot in the matrix shows how similar an event to all the others.\relax }}{29}{figure.caption.38}}
\newlabel{ch2:fig:4-1}{{8}{29}{For 20 events of MED-14, our textual embedding (right) is more discriminant than the LSI feature representation (left). Each dot in the matrix shows how similar an event to all the others.\relax }{figure.caption.38}{}}
\newlabel{ch2:eq:loss_model_v}{{\relax 2.6}{29}{Textual Embedding}{equation.40}{}}
\newlabel{ch2:fig:4-5-1}{{\caption@xref {ch2:fig:4-5-1}{ on input line 59}}{30}{Textual Embedding}{figure.caption.39}{}}
\newlabel{ch2:fig:4-5-2}{{\caption@xref {ch2:fig:4-5-2}{ on input line 62}}{30}{Textual Embedding}{figure.caption.39}{}}
\newlabel{ch2:fig:4-5-3}{{\caption@xref {ch2:fig:4-5-3}{ on input line 65}}{30}{Textual Embedding}{figure.caption.39}{}}
\newlabel{ch2:fig:4-5-4}{{\caption@xref {ch2:fig:4-5-4}{ on input line 68}}{30}{Textual Embedding}{figure.caption.39}{}}
\newlabel{ch2:fig:4-5-5}{{\caption@xref {ch2:fig:4-5-5}{ on input line 71}}{30}{Textual Embedding}{figure.caption.39}{}}
\newlabel{ch2:fig:4-5-6}{{\caption@xref {ch2:fig:4-5-6}{ on input line 74}}{30}{Textual Embedding}{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces We visualize the results of video embedding using the unified embedding model$^\mathcal  {U}$ and baselines model$^\mathcal  {V}$, model$^\mathcal  {S}$. Each sub-figure shows how discriminant the representation of the embedded videos. Each dot represents a projected video, while each pentagon-shape represents a projected event description. We use t-SNE to visualize the result.\relax }}{30}{figure.caption.39}}
\newlabel{ch2:fig:4-5}{{9}{30}{We visualize the results of video embedding using the unified embedding model$^\mathcal {U}$ and baselines model$^\mathcal {V}$, model$^\mathcal {S}$. Each sub-figure shows how discriminant the representation of the embedded videos. Each dot represents a projected video, while each pentagon-shape represents a projected event description. We use t-SNE to visualize the result.\relax }{figure.caption.39}{}}
\newlabel{ch2:eq:loss_model_c}{{\relax 2.7}{30}{Textual Embedding}{equation.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Unified Embedding and Metric Learning}{30}{subsection.42}}
\newlabel{ch2:eq:loss_model_n}{{\relax 2.8}{30}{Unified Embedding and Metric Learning}{equation.43}{}}
\citation{maaten2008visualizing}
\citation{ye2015eventnet}
\citation{over2013trecvid,over2014trecvid,jiang2011consumer}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Mitigating Noise in EventNet}{31}{subsection.44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.6}Latent Topics in LSI}{31}{subsection.45}}
\citation{mazloom2015tagbook}
\citation{chang2015semantic}
\citation{chang2016dynamic}
\citation{changthey}
\citation{habibian2015videostory}
\citation{habibian2015videostory}
\citation{mazloom2015tagbook}
\citation{chang2015semantic}
\citation{chang2016dynamic}
\citation{changthey}
\citation{habibian2015videostory}
\citation{mazloom2015tagbook}
\citation{chang2015semantic}
\citation{chang2016dynamic}
\citation{changthey}
\citation{habibian2015videostory}
\citation{habibian2015videostory}
\citation{changthey}
\citation{chang2016dynamic}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Similarity matrix between LSI features of MED-14 events. The more the latent topics $(K)$ in LSI model, the higher the feature dimension, and the more discriminant the feature.\relax }}{32}{figure.caption.46}}
\newlabel{ch2:fig:4-6}{{10}{32}{Similarity matrix between LSI features of MED-14 events. The more the latent topics $(K)$ in LSI model, the higher the feature dimension, and the more discriminant the feature.\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Results}{32}{section.47}}
\newlabel{ch2:sec:results}{{2.5}{32}{Results}{section.47}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison between the unified embedding and other baselines. The unified embedding model$^\mathcal  {U}$ achieves the best results on MED-13 and MED-14 datasets.\relax }}{32}{table.caption.48}}
\newlabel{ch2:tbl:5-3}{{1}{32}{Comparison between the unified embedding and other baselines. The unified embedding model$^\mathcal {U}$ achieves the best results on MED-13 and MED-14 datasets.\relax }{table.caption.48}{}}
\citation{habibian2015videostory}
\citation{jiang2017exploiting}
\citation{ye2015eventnet}
\citation{ye2015eventnet}
\citation{ye2015eventnet}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance comparison between our model and related works. We report the mean average precision (mAP\%) for MED-13 and MED-14 datasets.\relax }}{33}{table.caption.49}}
\newlabel{ch2:tbl:5-2}{{2}{33}{Performance comparison between our model and related works. We report the mean average precision (mAP\%) for MED-13 and MED-14 datasets.\relax }{table.caption.49}{}}
\newlabel{ch2:fig:5-1-1}{{\caption@xref {ch2:fig:5-1-1}{ on input line 62}}{33}{Results}{figure.caption.50}{}}
\newlabel{ch2:fig:5-1-2}{{\caption@xref {ch2:fig:5-1-2}{ on input line 65}}{33}{Results}{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Event detection accuracies: per-event average precision (AP\%) and per-dataset mean average precision (mAP\%) for MED-13 and MED-14 datasets. We compare our results against TagBook~\cite  {mazloom2015tagbook}, Discovary~\cite  {chang2015semantic}, Composition~\cite  {chang2016dynamic}, Classifiers~\cite  {changthey} and VideoStory~\cite  {habibian2015videostory}.\relax }}{33}{figure.caption.50}}
\newlabel{ch2:fig:5-1}{{11}{33}{Event detection accuracies: per-event average precision (AP\%) and per-dataset mean average precision (mAP\%) for MED-13 and MED-14 datasets. We compare our results against TagBook~\cite {mazloom2015tagbook}, Discovary~\cite {chang2015semantic}, Composition~\cite {chang2016dynamic}, Classifiers~\cite {changthey} and VideoStory~\cite {habibian2015videostory}.\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Conclusion}{33}{section.52}}
\newlabel{ch2:sec:conclusions}{{2.6}{33}{Conclusion}{section.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Our method improves over VideoStory when trained on the same dataset and using the same feature representation.\relax }}{34}{table.caption.51}}
\newlabel{ch2:tbl:5-1}{{3}{34}{Our method improves over VideoStory when trained on the same dataset and using the same feature representation.\relax }{table.caption.51}{}}
\citation{hussein2019timeception}
\citation{kuehne2011hmdb,soomro2012ucf101,kay2017kinetics}
\citation{sigurdsson2016hollywood,ye2015eventnet}
\citation{girdhar2017attentional,miech2017learnable}
\citation{donahue2015long,girdhar2017actionvlad}
\citation{ji20133d,tran2015learning,tran2018closer}
\citation{carreira2017quo}
\citation{wang2017non}
\citation{tran2015learning,xie2017rethinking}
\citation{sigurdsson2016hollywood}
\citation{kuehne2014language}
\citation{yeung2018every}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}\spacedlowsmallcaps  {Timeception for Complex Action Recognition}}{35}{chapter.53}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:timeception}{{3}{35}{\texorpdfstring {\spacedlowsmallcaps {Timeception for Complex Action Recognition}}{Timeception for Complex Action Recognition}}{chapter.53}{}}
\@writefile{toc}{\vskip 10pt}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{35}{section.54}}
\citation{hussein2017unified,habibian2017video2vec}
\citation{girdhar2017attentional}
\citation{fernando2017rank}
\citation{bilen2016dynamic}
\citation{miech2017learnable}
\citation{oneata2013action}
\citation{sanchez2013image}
\citation{girdhar2017actionvlad,cosmin2017spatio,duta2017spatio}
\citation{arandjelovic2013all}
\citation{donahue2015long}
\citation{ghodrati2018video}
\citation{huang2017densely}
\citation{karpathy2014large}
\citation{feichtenhofer2016convolutional}
\citation{simonyan2014two}
\citation{bilen2017action}
\citation{tran2015learning,ji20133d}
\citation{carreira2017quo}
\citation{sigurdsson2017asynchronous}
\citation{zhou2017temporal}
\citation{wang2016temporal,wang2018temporal}
\citation{varol2017long}
\citation{vaswani2017attention}
\citation{wang2017non}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Properties of complex action ``Cooking a Meal": \textbf  {\textit  {i.} composition}: consists of several one-actions (Cook, ...), \textbf  {\textit  {ii.} order}: weak temporal order of one-actions (Get {\relax \fontsize  {14.4}{18}\selectfont  $\llcurly $} Wash), \textbf  {\textit  {iii.} extent}: one-actions vary in their temporal extents.\relax }}{36}{figure.caption.56}}
\newlabel{ch3:fig:1-1}{{12}{36}{Properties of complex action ``Cooking a Meal": \textbf {\textit {i.} composition}: consists of several one-actions (Cook, ...), \textbf {\textit {ii.} order}: weak temporal order of one-actions (Get {\large $\llcurly $} Wash), \textbf {\textit {iii.} extent}: one-actions vary in their temporal extents.\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Related Work}{36}{section.57}}
\newlabel{ch3:sec:related}{{3.2}{36}{Related Work}{section.57}{}}
\citation{wang2015towards,karpathy2014large}
\citation{tran2015learning,ji20133d,feichtenhofer2018have,varol2017long,hara2017can}
\citation{he2016deep}
\citation{chollet2016xception}
\citation{xie2017rethinking,tran2018closer}
\citation{he2016deep}
\citation{zhang2017shufflenet}
\citation{xie2017aggregated}
\citation{szegedy2015going,szegedy2016rethinking}
\citation{tran2015learning,carreira2017quo}
\citation{xie2017rethinking}
\citation{xie2017rethinking}
\citation{tran2018closer,chollet2016xception}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Method}{37}{section.58}}
\newlabel{ch3:sec:model}{{3.3}{37}{Method}{section.58}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Motivation}{37}{subsection.59}}
\newlabel{subsec:motivation}{{3.3.1}{37}{Motivation}{subsection.59}{}}
\citation{he2016deep}
\citation{carreira2017quo}
\citation{carreira2017quo}
\citation{xie2017rethinking,tran2018closer}
\citation{carreira2017quo,xie2017rethinking,tran2018closer}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Timeception Layer}{38}{subsection.60}}
\citation{xie2017aggregated}
\citation{zhang2017shufflenet}
\citation{chollet2016xception}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The core component of our method is Timeception layer, left. Simply, it takes as an input the features $\mathbf  {X}$; corresponding to $T$ timesteps from the previous layer in the network. Then, it splits them into $N$ groups, and temporally convolves each group using temporal convolution module, right. It is a novel building block comprising multi-scale \textit  {temporal-only} convolutions to tolerate a variety of temporal extents in a complex action. Timeception makes use of grouped convolutions and channel shuffling to learn cross-channel correlations efficiently than $1 \times 1$ spatial convolutions.\relax }}{39}{figure.caption.61}}
\newlabel{ch3:fig:3-1}{{13}{39}{The core component of our method is Timeception layer, left. Simply, it takes as an input the features $\mathbf {X}$; corresponding to $T$ timesteps from the previous layer in the network. Then, it splits them into $N$ groups, and temporally convolves each group using temporal convolution module, right. It is a novel building block comprising multi-scale \textit {temporal-only} convolutions to tolerate a variety of temporal extents in a complex action. Timeception makes use of grouped convolutions and channel shuffling to learn cross-channel correlations efficiently than $1 \times 1$ spatial convolutions.\relax }{figure.caption.61}{}}
\citation{szegedy2015going}
\citation{van2016wavenet}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces To tolerate temporal extents, we use multi-scale temporal kernels, with two options: \textit  {i}. different kernel sizes $k \in \{1,3,5,7\}$ and fixed dilation rate $d=1$, \textit  {ii}. different dilation rates $d \in \{1,2,3\}$ and fixed kernel size $k=3$.\relax }}{40}{figure.caption.62}}
\newlabel{ch3:fig:3-4}{{14}{40}{To tolerate temporal extents, we use multi-scale temporal kernels, with two options: \textit {i}. different kernel sizes $k \in \{1,3,5,7\}$ and fixed dilation rate $d=1$, \textit {ii}. different dilation rates $d \in \{1,2,3\}$ and fixed kernel size $k=3$.\relax }{figure.caption.62}{}}
\citation{he2016deep}
\citation{carreira2017quo}
\citation{timeceptioncode}
\citation{tensorflow2015-whitepaper}
\citation{chollet2015keras}
\citation{sigurdsson2016hollywood}
\citation{kuehne2014language}
\citation{yeung2018every}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}The Final Model}{41}{subsection.63}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Experiments}{41}{section.64}}
\newlabel{ch3:sec:experiments}{{3.4}{41}{Experiments}{section.64}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Datasets}{41}{subsection.65}}
\citation{sigurdsson2017asynchronous,girdhar2017actionvlad,sigurdsson2017asynchronous,wang2017non,wang2018videos}
\citation{idrees2017thumos}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Tolerating Temporal Extents}{42}{subsection.66}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces  We split a video of 128 timesteps into segments of equal length (left, before alteration), and alter their temporal extents by expansion and shrinking (right, after alteration). We use 4 types of alterations:(a) very-coarse, (b) coarse, (c) fine, and (d) very-fine. Numbers in boxes are timesteps.\relax }}{42}{figure.caption.67}}
\newlabel{ch3:fig:4-1}{{15}{42}{We split a video of 128 timesteps into segments of equal length (left, before alteration), and alter their temporal extents by expansion and shrinking (right, after alteration). We use 4 types of alterations:(a) very-coarse, (b) coarse, (c) fine, and (d) very-fine. Numbers in boxes are timesteps.\relax }{figure.caption.67}{}}
\citation{tran2015learning,carreira2017quo,wang2017non,xie2017rethinking}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces  Timeception, with multi-scale kernel, tolerates the altered temporal extents better than fixed-size kernels. We report the percentage drop in mAP (lower is better) when testing on original \textit  {v.s.} altered videos of Charades. I3D and ResNet are backbone CNNs.\relax }}{43}{table.caption.68}}
\newlabel{ch3:tbl:4-1}{{4}{43}{Timeception, with multi-scale kernel, tolerates the altered temporal extents better than fixed-size kernels. We report the percentage drop in mAP (lower is better) when testing on original \textit {v.s.} altered videos of Charades. I3D and ResNet are backbone CNNs.\relax }{table.caption.68}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces  Timeception, using multi-scale kernels (i.e. different kernel sizes ($k$) or dilation rates ($d$), outperforms fixed-size kernels on Charades. I3D/ResNet are backbone.\relax }}{43}{table.caption.69}}
\newlabel{ch3:tbl:4-2}{{5}{43}{Timeception, using multi-scale kernels (i.e. different kernel sizes ($k$) or dilation rates ($d$), outperforms fixed-size kernels on Charades. I3D/ResNet are backbone.\relax }{table.caption.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The learned weights by temporal convolutions of three Timeception layers. Each uses multi-scale convolutions with varying kernel sizes $k \in \{3, 5, 7\}$. In bottom layer (1), we notice that long kernels ($k=7$) captures fine-grained temporal dependencies. But at the top layer (3), the long kernels tend to focus on coarse-grained temporal correlation. The same behavior prevails for the shot ($k=3$) and medium ($k=5$) kernels.\relax }}{44}{figure.caption.70}}
\newlabel{ch3:fig:4-5}{{16}{44}{The learned weights by temporal convolutions of three Timeception layers. Each uses multi-scale convolutions with varying kernel sizes $k \in \{3, 5, 7\}$. In bottom layer (1), we notice that long kernels ($k=7$) captures fine-grained temporal dependencies. But at the top layer (3), the long kernels tend to focus on coarse-grained temporal correlation. The same behavior prevails for the shot ($k=3$) and medium ($k=5$) kernels.\relax }{figure.caption.70}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Long-range Temporal Dependencies}{44}{subsection.71}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces  Timeception layers allow for deep and efficient temporal models, able to learn the temporal abstractions needed to learn complex actions. Columns are: \textit  {Baseline}: backbone CNN + how many Timception layers (TC) on top of it, \textit  {CNN Steps}: input timesteps to the CNN, \textit  {TC Steps}: input timesteps to the first Timeception layer, \textit  {Params}: number of parameters used by Timeception layers, in millions.\relax }}{44}{table.caption.72}}
\newlabel{ch3:tbl:4-3}{{6}{44}{Timeception layers allow for deep and efficient temporal models, able to learn the temporal abstractions needed to learn complex actions. Columns are: \textit {Baseline}: backbone CNN + how many Timception layers (TC) on top of it, \textit {CNN Steps}: input timesteps to the CNN, \textit {TC Steps}: input timesteps to the first Timeception layer, \textit {Params}: number of parameters used by Timeception layers, in millions.\relax }{table.caption.72}{}}
\citation{tran2018closer}
\citation{sigurdsson2017asynchronous}
\citation{sigurdsson2017asynchronous}
\citation{girdhar2017actionvlad}
\citation{sigurdsson2017asynchronous}
\citation{zhou2017temporal}
\citation{charades2017algorithms}
\citation{carreira2017quo}
\citation{wang2017non}
\citation{wang2017non}
\citation{wang2018videos}
\citation{wang2018videos}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Effectiveness of Timeception}{45}{subsection.73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Experiments on Benchmarks}{45}{subsection.75}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  Top: the cost of adding new Timeception layers is marginal, compared to related temporal layers. Bottom: Timeception improves performance, scales up temporal capacity of backbone CNNs while maintaining the model size.\relax }}{46}{figure.caption.74}}
\newlabel{ch3:fig:4-9}{{17}{46}{Top: the cost of adding new Timeception layers is marginal, compared to related temporal layers. Bottom: Timeception improves performance, scales up temporal capacity of backbone CNNs while maintaining the model size.\relax }{figure.caption.74}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces  Timeception (TC) outperforms related works using the same backbone CNN. It achieves the absolute gain of $8.8\%$ and $4.3\%$ over ResNet and I3D, respectively. More over, using the full capacity of Timeception improves $1.4\%$ over best related work.\relax }}{46}{table.caption.76}}
\newlabel{ch3:tbl:4-6}{{7}{46}{Timeception (TC) outperforms related works using the same backbone CNN. It achieves the absolute gain of $8.8\%$ and $4.3\%$ over ResNet and I3D, respectively. More over, using the full capacity of Timeception improves $1.4\%$ over best related work.\relax }{table.caption.76}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  Multi-scale Timeception outperforms the fixed-kernel when complex actions are dynamic, in green. But when complex actions with rigid temporal patters, fixed-size performs better than multi-scale, in orange.\relax }}{47}{figure.caption.77}}
\newlabel{ch3:fig:4-7}{{18}{47}{Multi-scale Timeception outperforms the fixed-kernel when complex actions are dynamic, in green. But when complex actions with rigid temporal patters, fixed-size performs better than multi-scale, in orange.\relax }{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  Long-range Timeception outperforms the short-range version when complex actions need the entire video to unfold, in green. However, we see one-actions that short-range Timeception can easily capture, in orange.\relax }}{47}{figure.caption.78}}
\newlabel{ch3:fig:4-8}{{19}{47}{Long-range Timeception outperforms the short-range version when complex actions need the entire video to unfold, in green. However, we see one-actions that short-range Timeception can easily capture, in orange.\relax }{figure.caption.78}{}}
\citation{scikit-learn}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces  Timeception outperform baselines in recognizing the long-range activities of Breakfast dataset.\relax }}{48}{table.caption.79}}
\newlabel{ch3:tbl:4-7}{{8}{48}{Timeception outperform baselines in recognizing the long-range activities of Breakfast dataset.\relax }{table.caption.79}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces  Timeception, with multi-scale temporal kernels, helps baseline models to capture the long-range dependencies between one-actions in videos of MultiTHUMOS.\relax }}{48}{table.caption.80}}
\newlabel{ch3:tbl:4-8}{{9}{48}{Timeception, with multi-scale temporal kernels, helps baseline models to capture the long-range dependencies between one-actions in videos of MultiTHUMOS.\relax }{table.caption.80}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Conclusion}{48}{section.81}}
\newlabel{ch3:sec:conclusions}{{3.5}{48}{Conclusion}{section.81}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}\spacedlowsmallcaps  {VideoGraph: Recognizing Minutes-Long Human Activities in Videos}}{51}{chapter.82}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:videograph}{{4}{51}{\texorpdfstring {\spacedlowsmallcaps {VideoGraph: Recognizing Minutes-Long Human Activities in Videos}}{VideoGraph: Recognizing Minutes-Long Human Activities in Videos}}{chapter.82}{}}
\@writefile{toc}{\vskip 10pt}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{51}{section.83}}
\newlabel{ch7:sec:introduction}{{4.1}{51}{Introduction}{section.83}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Related Work}{51}{section.84}}
\newlabel{ch7:sec:related}{{4.2}{51}{Related Work}{section.84}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Method}{51}{section.85}}
\newlabel{ch7:sec:model}{{4.3}{51}{Method}{section.85}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Experiments}{51}{section.86}}
\newlabel{ch7:sec:experiments}{{4.4}{51}{Experiments}{section.86}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Conclusion}{51}{section.87}}
\citation{kuehne2014language}
\citation{hussein2019timeception}
\citation{hussein2019videograph}
\citation{kuehne2014language}
\citation{hussein2020pic}
\citation{hussein2019timeception,carreira2017quo}
\citation{carreira2017quo}
\citation{hussein2019timeception,lea2017temporal}
\citation{duta2017spatio,girdhar2017actionvlad}
\citation{wang2017non,wu2019long}
\citation{vaswani2017attention}
\citation{santoro2016meta,vinyals2016matching,finn2017model}
\citation{carreira2017quo}
\citation{duta2017spatio,girdhar2017actionvlad}
\citation{wang2017non}
\citation{wang2017non}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}\spacedlowsmallcaps  {Permutation Invariant Convolution for Recognizing Long-range Activities}}{53}{chapter.88}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:pic}{{5}{53}{\texorpdfstring {\spacedlowsmallcaps {Permutation Invariant Convolution for Recognizing Long-range Activities}}{Permutation Invariant Convolution for Recognizing Long-range Activities}}{chapter.88}{}}
\@writefile{toc}{\vskip 10pt}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{53}{section.89}}
\newlabel{ch4:sec:introduction}{{5.1}{53}{Introduction}{section.89}{}}
\citation{soomro2012ucf101}
\citation{karpathy2014large}
\citation{kay2017kinetics}
\citation{lea2017temporal}
\citation{ghodrati2018video}
\citation{wang2011action,jain2013better}
\citation{hussein2019timeception}
\citation{kuehne2014language}
\citation{hussein2019videograph}
\citation{sigurdsson2016hollywood}
\citation{damen2018scaling}
\citation{kuehne2014language}
\citation{yeung2015every,idrees2017thumos}
\citation{zhou2018towards}
\citation{sener2019zero}
\citation{hussein2019timeception}
\citation{wang2017non,wu2019long}
\citation{girdhar2017actionvlad,duta2017spatio}
\citation{krizhevsky2014imagenet,simonyan2014very}
\citation{krizhevsky2014imagenet,he2016deep}
\citation{ji20123d,simonyan2014two}
\citation{duta2017spatio}
\citation{gehring2017convolutional}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces PIC, Permutation Invariant Convolution, recognizes long-range activities using multiple levels of abstractions. On the micro-level of a short segment $s_1$, PIC models the correlation between unit-actions, regardless of their order, repetition or duration, $s_1$$=$$\{ \tmspace  +\thinmuskip {.1667em}\tmspace  +\thinmuskip {.1667em} \text  {\ActionCircleB  } \tmspace  +\thinmuskip {.1667em}\tmspace  +\thinmuskip {.1667em}, \tmspace  +\thinmuskip {.1667em}\tmspace  +\thinmuskip {.1667em} \text  {\ActionCircleA  } \tmspace  +\thinmuskip {.1667em}\tmspace  +\thinmuskip {.1667em} \} = \{ \tmspace  +\thinmuskip {.1667em}\tmspace  +\thinmuskip {.1667em} \text  {\ActionCircleA  } \tmspace  +\thinmuskip {.1667em}\tmspace  +\thinmuskip {.1667em}, \tmspace  +\thinmuskip {.1667em}\tmspace  +\thinmuskip {.1667em} \text  {\ActionCircleB  } \tmspace  +\thinmuskip {.1667em}\tmspace  +\thinmuskip {.1667em}, \tmspace  +\thinmuskip {.1667em}\tmspace  +\thinmuskip {.1667em} \text  {\ActionCircleA  } \tmspace  +\thinmuskip {.1667em}\tmspace  +\thinmuskip {.1667em} \}$. On the macro-level, PIC learns the interactions between segments.\relax }}{54}{figure.caption.91}}
\newlabel{ch4:fig:1-1}{{20}{54}{PIC, Permutation Invariant Convolution, recognizes long-range activities using multiple levels of abstractions. On the micro-level of a short segment $s_1$, PIC models the correlation between unit-actions, regardless of their order, repetition or duration, $s_1$$=$$\{ \,\, \text {\ActionCircleB } \,\,, \,\, \text {\ActionCircleA } \,\, \} = \{ \,\, \text {\ActionCircleA } \,\,, \,\, \text {\ActionCircleB } \,\,, \,\, \text {\ActionCircleA } \,\, \}$. On the macro-level, PIC learns the interactions between segments.\relax }{figure.caption.91}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Related Work}{54}{section.92}}
\newlabel{ch4:sec:related}{{5.2}{54}{Related Work}{section.92}{}}
\citation{hussein2019timeception,szegedy2017inception}
\citation{xu2015show}
\citation{sharma2015action}
\citation{du2018interaction,li2018videolstm}
\citation{vaswani2017attention}
\citation{velivckovic2017graph}
\citation{wang2017non}
\citation{wu2019long,girdhar2019video}
\citation{wang2017non}
\citation{parmar2019stand}
\citation{lample2019large}
\citation{lample2019large}
\citation{hussein2017unified}
\citation{girdhar2017attentional}
\citation{miech2017learnable}
\citation{duta2017spatio,girdhar2017actionvlad}
\citation{oneata2013action}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Compared to other temporal modeling layers, PIC has three benefits. \textit  {i.} Temporal locality to learn long-range temporal abstractions using a cascade of layers. \textit  {ii.} Shared weights (i.e. key-value kernels) to detect the discriminant concepts. \textit  {iii.} Invariant to the temporal permutation with in the receptive field, better for modeling weak structures.\relax }}{55}{figure.caption.93}}
\newlabel{ch4:fig:2-1}{{21}{55}{Compared to other temporal modeling layers, PIC has three benefits. \textit {i.} Temporal locality to learn long-range temporal abstractions using a cascade of layers. \textit {ii.} Shared weights (i.e. key-value kernels) to detect the discriminant concepts. \textit {iii.} Invariant to the temporal permutation with in the receptive field, better for modeling weak structures.\relax }{figure.caption.93}{}}
\citation{hussein2019timeception}
\citation{wang2017non}
\citation{girdhar2017actionvlad}
\citation{hussein2019timeception}
\citation{hussein2019timeception}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Method}{56}{section.94}}
\newlabel{ch4:sec:model}{{5.3}{56}{Method}{section.94}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Motivation}{56}{subsection.95}}
\citation{wang2017non,vaswani2017attention}
\citation{vaswani2017attention}
\citation{wang2017non}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  Overview of PIC, Permutation Invariant Convolution. Using a pair of Key-Value kernels $(K, V)$, it models the correlation between the visual evidences $\{ \tmspace  +\thickmuskip {.2777em} \textbf  {\ActionCircleA  } \tmspace  +\thickmuskip {.2777em}, \tmspace  +\thickmuskip {.2777em} \textbf  {\ActionCircleB  } \tmspace  +\thickmuskip {.2777em}, \tmspace  +\thickmuskip {.2777em} \textbf  {\ActionCircleE  } \tmspace  +\thickmuskip {.2777em} \}$ in a local window with $X_w = \{ x_1, ..., x_T \}$ irrespective of their the temporal order.\relax }}{57}{figure.caption.97}}
\newlabel{ch4:fig:3-1}{{22}{57}{Overview of PIC, Permutation Invariant Convolution. Using a pair of Key-Value kernels $(K, V)$, it models the correlation between the visual evidences $\{ \; \textbf {\ActionCircleA } \;, \; \textbf {\ActionCircleB } \;, \; \textbf {\ActionCircleE } \; \}$ in a local window with $X_w = \{ x_1, ..., x_T \}$ irrespective of their the temporal order.\relax }{figure.caption.97}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}PIC: Permutation Invariant Convolution}{57}{subsection.98}}
\citation{hussein2019timeception,wang2017non,xie2017aggregated}
\citation{he2016deep,xie2017aggregated}
\citation{wang2017non,vaswani2017attention}
\citation{hussein2019timeception}
\citation{wang2017non}
\citation{wang2017non}
\newlabel{ch4:eqn:3-2-1}{{\relax 5.2}{58}{PIC: Permutation Invariant Convolution}{equation.99}{}}
\newlabel{ch4:eqn:3-2-2}{{\relax 5.3}{58}{PIC: Permutation Invariant Convolution}{equation.100}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces PIC layer models only the temporal dimension. It has shared kernels $K,V$ to learn discriminant concepts. A residual bottleneck is used to reduce computation.\relax }}{58}{figure.caption.103}}
\newlabel{ch4:fig:3-2}{{23}{58}{PIC layer models only the temporal dimension. It has shared kernels $K,V$ to learn discriminant concepts. A residual bottleneck is used to reduce computation.\relax }{figure.caption.103}{}}
\citation{he2016deep}
\citation{carreira2017quo}
\citation{carreira2017quo}
\citation{tensorflow2015-whitepaper}
\citation{chollet2015keras}
\citation{sigurdsson2016hollywood}
\citation{kuehne2014language}
\citation{yeung2015every}
\citation{wang2017non}
\citation{girdhar2017actionvlad}
\citation{hussein2019timeception}
\citation{sigurdsson2016hollywood}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Final Model}{59}{subsection.104}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Experiments}{59}{section.105}}
\newlabel{ch4:sec:experiments}{{5.4}{59}{Experiments}{section.105}{}}
\citation{kuehne2014language}
\citation{yeung2015every}
\citation{hussein2019timeception}
\citation{hussein2019timeception}
\citation{yeung2015every}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Datasets}{60}{subsection.106}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Dissection of PIC}{60}{subsection.107}}
\citation{parmar2019stand}
\citation{wang2017non,vaswani2017attention}
\citation{wang2017non}
\citation{wang2017non}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces  Three different ways of sampling timesteps from a test video: uniform, coarse, and fine permutation.\relax }}{61}{figure.caption.109}}
\newlabel{ch4:fig:4-1}{{24}{61}{Three different ways of sampling timesteps from a test video: uniform, coarse, and fine permutation.\relax }{figure.caption.109}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Being invariant to permutations, PIC is affected the least by altering the temporal order of test videos.\relax }}{61}{table.caption.110}}
\newlabel{ch4:tbl:4-4}{{10}{61}{Being invariant to permutations, PIC is affected the least by altering the temporal order of test videos.\relax }{table.caption.110}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Having a local receptive field enables PIC to learn levels of abstractions at multiple layers. Thus, improving monotonically by stacking more layers. Others don't witness the same benefit, as they use global receptive field.\relax }}{62}{table.caption.111}}
\newlabel{ch4:tbl:4-5}{{11}{62}{Having a local receptive field enables PIC to learn levels of abstractions at multiple layers. Thus, improving monotonically by stacking more layers. Others don't witness the same benefit, as they use global receptive field.\relax }{table.caption.111}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Thanks to sharing the kernels $(K, V)$, PIC is better at learning concepts than layers with inferred kernels.\relax }}{62}{table.caption.113}}
\newlabel{ch4:tbl:4-6}{{12}{62}{Thanks to sharing the kernels $(K, V)$, PIC is better at learning concepts than layers with inferred kernels.\relax }{table.caption.113}{}}
\citation{hussein2019timeception}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Analysis of PIC}{63}{subsection.114}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces  On x-axis, the number of stacked layers. While on y-axes, the the efficiency of temporal layers using four metrics: \textit  {i.} CPU feedforward time (milliseconds), \textit  {ii.} model parameters (millions), \textit  {iii.} number of operations (mega FLOPS), and \textit  {iv.} classification accuracy (\%). PIC has the best tradeoff between efficiency and effectiveness.\relax }}{63}{figure.caption.115}}
\newlabel{ch4:fig:4-2}{{25}{63}{On x-axis, the number of stacked layers. While on y-axes, the the efficiency of temporal layers using four metrics: \textit {i.} CPU feedforward time (milliseconds), \textit {ii.} model parameters (millions), \textit {iii.} number of operations (mega FLOPS), and \textit {iv.} classification accuracy (\%). PIC has the best tradeoff between efficiency and effectiveness.\relax }{figure.caption.115}{}}
\citation{hussein2019timeception}
\citation{hussein2019timeception}
\citation{hussein2019timeception}
\citation{arandjelovic2013all}
\citation{wang2017non}
\citation{hussein2019timeception}
\citation{hussein2019timeception}
\citation{hussein2019timeception}
\citation{hussein2019timeception}
\citation{wu2019long}
\citation{wang2017non}
\citation{feichtenhofer2019slowfast}
\citation{feichtenhofer2019slowfast}
\citation{wang2017non}
\citation{hussein2019timeception}
\citation{wu2019long}
\citation{wu2019long}
\citation{hussein2019timeception}
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces PIC accuracy when changing the convolution window size $T$ and the downsampling stride size $s$.\relax }}{64}{table.caption.116}}
\newlabel{ch4:tbl:4-3}{{13}{64}{PIC accuracy when changing the convolution window size $T$ and the downsampling stride size $s$.\relax }{table.caption.116}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.4}Quantitative Analysis}{64}{subsection.117}}
\@writefile{lot}{\contentsline {table}{\numberline {14}{\ignorespaces We report the accuracy of classifying the minutes-long activities of Breakfast. PIC outperforms the other baseline methods by a considerable margin. $\ast $ denotes that the backbone CNN is not-fined tuned, so we can be compared with~\cite  {hussein2019timeception}.\relax }}{64}{table.caption.118}}
\newlabel{ch4:tbl:5-1}{{14}{64}{We report the accuracy of classifying the minutes-long activities of Breakfast. PIC outperforms the other baseline methods by a considerable margin. $\ast $ denotes that the backbone CNN is not-fined tuned, so we can be compared with~\cite {hussein2019timeception}.\relax }{table.caption.118}{}}
\citation{hussein2019timeception}
\@writefile{lot}{\contentsline {table}{\numberline {15}{\ignorespaces When classifying the complex multi-label actions of Charades, PIC layers bring improvements over previous works.\relax }}{65}{table.caption.119}}
\newlabel{ch4:tbl:5-2}{{15}{65}{When classifying the complex multi-label actions of Charades, PIC layers bring improvements over previous works.\relax }{table.caption.119}{}}
\@writefile{lot}{\contentsline {table}{\numberline {16}{\ignorespaces PIC improves over related works in recognizing the multi-labeled, long-range videos of MultiThumos.\relax }}{65}{table.caption.120}}
\newlabel{ch4:tbl:4-7}{{16}{65}{PIC improves over related works in recognizing the multi-labeled, long-range videos of MultiThumos.\relax }{table.caption.120}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.5}Qualitative Analysis}{65}{subsection.121}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces  In a cascade of PIC layers, we notice that in the bottom layer, the learned concepts are fine-grained and independent of the activity category. For example, the concept ``pouring" is irrespective of activities ``make coffee" or ``make tea". Also, these concepts can be object-centric as ``food box" or action-centric as ``cutting".\relax }}{66}{figure.caption.122}}
\newlabel{ch4:fig:4-4}{{26}{66}{In a cascade of PIC layers, we notice that in the bottom layer, the learned concepts are fine-grained and independent of the activity category. For example, the concept ``pouring" is irrespective of activities ``make coffee" or ``make tea". Also, these concepts can be object-centric as ``food box" or action-centric as ``cutting".\relax }{figure.caption.122}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces  This figure shows 16 frames uniformly sampled from an activity of ``Making Pancake". After one layer, $M$ concept kernels are learned to detect relevant visual evidences. For simplicity, we show the activations of only one kernel per layer. In red, the activations of one kernel in bottom layer. While in blue, the activations of another kernel in the top layer.\relax }}{66}{figure.caption.123}}
\newlabel{ch4:fig:4-5}{{27}{66}{This figure shows 16 frames uniformly sampled from an activity of ``Making Pancake". After one layer, $M$ concept kernels are learned to detect relevant visual evidences. For simplicity, we show the activations of only one kernel per layer. In red, the activations of one kernel in bottom layer. While in blue, the activations of another kernel in the top layer.\relax }{figure.caption.123}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Conclusion}{66}{section.124}}
\newlabel{ch4:sec:conclusions}{{5.5}{66}{Conclusion}{section.124}{}}
\citation{szelag2004individual}
\citation{hussein2020timegate}
\citation{howard2017mobilenets,zhang2018shufflenet,zoph2018learning}
\citation{kopuklu2019resource}
\citation{kay2017kinetics}
\citation{soomro2012ucf101}
\citation{kuehne2011hmdb}
\citation{schindler2008action}
\citation{sigurdsson2016hollywood}
\citation{kuehne2014language}
\citation{yeung2018every}
\citation{wang2018non,carreira2017quo}
\citation{yeung2016end}
\citation{korbar2019scsampler}
\citation{kay2017kinetics}
\citation{karpathy2014large}
\citation{korbar2019scsampler}
\citation{hussein2019timeception,ye2015eventnet}
\citation{carreira2017quo,he2016deep}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}\spacedlowsmallcaps  {Timegate: Conditional Gating of Segments in Long-range Activities}}{69}{chapter.125}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:timegate}{{6}{69}{\texorpdfstring {\spacedlowsmallcaps {Timegate: Conditional Gating of Segments in Long-range Activities}}{Timegate: Conditional Gating of Segments in Long-range Activities}}{chapter.125}{}}
\@writefile{toc}{\vskip 10pt}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{69}{section.126}}
\newlabel{ch5:sec:introduction}{{6.1}{69}{Introduction}{section.126}{}}
\citation{sigurdsson2016hollywood}
\citation{kuehne2014language}
\citation{yeung2018every}
\citation{kay2017kinetics}
\citation{soomro2012ucf101}
\citation{wang2016temporal}
\citation{schindler2008action}
\citation{sigurdsson2016hollywood}
\citation{kuehne2014language}
\citation{hussein2019timeception,hussein2019videograph}
\citation{bhardwaj2019efficient}
\citation{hassibi1993optimal,han2015learning}
\citation{li2016pruning}
\citation{howard2019searching}
\citation{zhang2018shufflenet}
\citation{kopuklu2019resource}
\citation{zoph2016neural}
\citation{zoph2018learning}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Top, short-range action ``disc throw" in an untrimmed video. Based on each segment, you can tell whether is it relevant (green) to the action or not (red). But in long-range activities, middle and bottom, the importance of each segment is conditioned on the video context. The segment ``get food from cupboard" is relevant to ``cook food" but not to ``washing dishes".\relax }}{70}{figure.caption.128}}
\newlabel{ch5:fig:1-1}{{28}{70}{Top, short-range action ``disc throw" in an untrimmed video. Based on each segment, you can tell whether is it relevant (green) to the action or not (red). But in long-range activities, middle and bottom, the importance of each segment is conditioned on the video context. The segment ``get food from cupboard" is relevant to ``cook food" but not to ``washing dishes".\relax }{figure.caption.128}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Related Work}{70}{section.129}}
\newlabel{ch5:sec:related}{{6.2}{70}{Related Work}{section.129}{}}
\citation{jang2016categorical}
\citation{veit2018convolutional}
\citation{chen2019you,bejnordi2019batch}
\citation{chen2019you}
\citation{jang2016categorical}
\citation{bhardwaj2019efficient}
\citation{yeung2016end}
\citation{korbar2019scsampler}
\citation{karpathy2014large}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Method}{71}{section.130}}
\newlabel{ch5:sec:model}{{6.3}{71}{Method}{section.130}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}TimeGate}{71}{subsection.132}}
\citation{hussein2019timeception}
\citation{girdhar2017actionvlad}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Overview of the proposed model, TimeGate, with two stages. The first stage is the timestep selector, left. Based on a lightweight CNN, \textit  {LightNet}, the model learns to select the most relevant timesteps for classifying the video. This selection is conditioned on both the features of timestep and its context. The second stage is the video classifier, right. In which, only the selected timesteps (\leavevmode {\color  {ForestGreen}\makebox  [0pt][l]{$\square $}\raisebox  {.15ex}{\hspace  {0.1em}$\boldsymbol  {\text  {$\mathsurround \z@ \mathchar "558$}}$}}) are considered, while the unselected timesteps (\leavevmode {\color  {BrickRed}{\fontfamily  {mvs}\fontencoding  {U}\fontseries  {m}\fontshape  {n}\selectfont  \char 88}}) are completely ignored. In this stage, a heavyweight CNN, \textit  {HeavyNet} is used for feature representation of only the selected timesteps, followed by MLP for classification.\relax }}{72}{figure.caption.131}}
\newlabel{ch5:fig:3-1}{{29}{72}{Overview of the proposed model, TimeGate, with two stages. The first stage is the timestep selector, left. Based on a lightweight CNN, \textit {LightNet}, the model learns to select the most relevant timesteps for classifying the video. This selection is conditioned on both the features of timestep and its context. The second stage is the video classifier, right. In which, only the selected timesteps (\textcolor {ForestGreen}{\CheckBoxCustom }) are considered, while the unselected timesteps (\textcolor {BrickRed}{\CrossedBox }) are completely ignored. In this stage, a heavyweight CNN, \textit {HeavyNet} is used for feature representation of only the selected timesteps, followed by MLP for classification.\relax }{figure.caption.131}{}}
\citation{jang2016categorical}
\citation{bejnordi2019batch}
\citation{bejnordi2019batch}
\citation{bejnordi2019batch}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Bottom, the timestep selector learns concept kernels $K$ to represent the most representative visual evidence. Top, the gating module learns to select only a timestep feature $x_i$ according to its importance to the current video.\relax }}{73}{figure.caption.133}}
\newlabel{ch5:fig:3-2}{{30}{73}{Bottom, the timestep selector learns concept kernels $K$ to represent the most representative visual evidence. Top, the gating module learns to select only a timestep feature $x_i$ according to its importance to the current video.\relax }{figure.caption.133}{}}
\citation{wang2018non}
\citation{bejnordi2019batch,louizos2017learning}
\citation{wang2018non}
\citation{sandler2018mobilenetv2}
\citation{carreira2017quo}
\citation{kopuklu2019resource}
\citation{he2016deep}
\newlabel{ch5:fig:3-3}{{\caption@xref {ch5:fig:3-3}{ on input line 135}}{74}{TimeGate}{figure.caption.134}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces  In training, we use \texttt  {gated-sigmoid} to activate the gating value $\alpha _i$ and to select the timesteps. \texttt  {gated-sigmoid} has some desirable properties. \textit  {i}. Unlike \texttt  {ReLU}, having upper bound does not allow a timestep feature to dominate others. \textit  {ii}. Different from \texttt  {sigmoid}, being clipped allows the network to discard insignificant timesteps, \textit  {i.e.} those with gating values $\alpha _i < 0.5$. In test, we replace the \texttt  {gated-sigmoid} with \texttt  {step function} for binary gating of timesteps.\relax }}{74}{figure.caption.134}}
\citation{hussein2019videograph}
\citation{yeung2018every}
\citation{hussein2019timeception}
\citation{hussein2019timeception}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}TimeGate Implementation}{75}{subsection.135}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Experiments}{75}{section.136}}
\newlabel{ch5:sec:experiments}{{6.4}{75}{Experiments}{section.136}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Datasets}{75}{subsection.137}}
\newlabel{ch5:subsec:4-1}{{6.4.1}{75}{Datasets}{subsection.137}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Our stand-alone timestep selector helps improving the performance and reduces the computation of off-the-shelf CNN classifiers -- be it 2D/3D heavyweight CNN or even lightweight 3D CNN. More over, if TimeGate is trained end-to-end, the selector learns a better gating to the benefit of the classifier. So, the performance is improved even further.\relax }}{76}{figure.caption.138}}
\newlabel{ch5:fig:4-1}{{32}{76}{Our stand-alone timestep selector helps improving the performance and reduces the computation of off-the-shelf CNN classifiers -- be it 2D/3D heavyweight CNN or even lightweight 3D CNN. More over, if TimeGate is trained end-to-end, the selector learns a better gating to the benefit of the classifier. So, the performance is improved even further.\relax }{figure.caption.138}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Stand-alone Timestep Selector}{76}{subsection.139}}
\newlabel{ch5:subsec:4-2}{{6.4.2}{76}{Stand-alone Timestep Selector}{subsection.139}{}}
\citation{korbar2019scsampler}
\citation{korbar2019scsampler}
\@writefile{lot}{\contentsline {table}{\numberline {17}{\ignorespaces The stand-alone selector of our model TimeGate (\textbf  {TG}) benefits off-the-shelf CNN classifiers. The benefit is consistent for various classifiers: I3D, ShuffleNet3D (S3D), and ResNet2D (R2D).\relax }}{77}{table.caption.140}}
\newlabel{ch5:tbl:4-1}{{17}{77}{The stand-alone selector of our model TimeGate (\textbf {TG}) benefits off-the-shelf CNN classifiers. The benefit is consistent for various classifiers: I3D, ShuffleNet3D (S3D), and ResNet2D (R2D).\relax }{table.caption.140}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}End-to-End TimeGate}{77}{subsection.141}}
\newlabel{ch5:subsec:4-3}{{6.4.3}{77}{End-to-End TimeGate}{subsection.141}{}}
\citation{korbar2019scsampler}
\citation{korbar2019scsampler}
\@writefile{lot}{\contentsline {table}{\numberline {18}{\ignorespaces Our stand-alone (\textbf  {SA}) selector benefits off-the-shelf CNN classifiers. End-to-end (\textbf  {ETE}) training is even better.\relax }}{78}{table.caption.142}}
\newlabel{ch5:tbl:4-4}{{18}{78}{Our stand-alone (\textbf {SA}) selector benefits off-the-shelf CNN classifiers. End-to-end (\textbf {ETE}) training is even better.\relax }{table.caption.142}{}}
\@writefile{lot}{\contentsline {table}{\numberline {19}{\ignorespaces TimeGate (TG) is better when the gating module is conditioned on both the frame-level and the context-level. More over, TimeGate outperforms SCSampler in long-range activities.\relax }}{78}{table.caption.145}}
\newlabel{ch5:tbl:4-2}{{19}{78}{TimeGate (TG) is better when the gating module is conditioned on both the frame-level and the context-level. More over, TimeGate outperforms SCSampler in long-range activities.\relax }{table.caption.145}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces  The ratios of selected timesteps for the activity classes of Breakfast. Note the change in these ratios from stand-alone selector (red) to end-to-end training with the HeavyNets: ResNet2D (green) I3D (yellow) and ShuffleNet3D (blue).\relax }}{78}{figure.caption.143}}
\newlabel{ch5:fig:4-5}{{33}{78}{The ratios of selected timesteps for the activity classes of Breakfast. Note the change in these ratios from stand-alone selector (red) to end-to-end training with the HeavyNets: ResNet2D (green) I3D (yellow) and ShuffleNet3D (blue).\relax }{figure.caption.143}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.4}Context-Conditional Gating}{78}{subsection.144}}
\newlabel{ch5:subsec:4-4}{{6.4.4}{78}{Context-Conditional Gating}{subsection.144}{}}
\citation{korbar2019scsampler}
\citation{korbar2019scsampler}
\citation{korbar2019scsampler}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces In both the frame-conditioned TimeGate and SCSampler, the ratio of the selected timesteps have small variance across the activity classes of Breakfast. In contrast, in context-conditioned TimeGate, the ratio is highly dependent on the activity, which means context-conditional gating is archived.\relax }}{79}{figure.caption.146}}
\newlabel{ch5:fig:4-6}{{34}{79}{In both the frame-conditioned TimeGate and SCSampler, the ratio of the selected timesteps have small variance across the activity classes of Breakfast. In contrast, in context-conditioned TimeGate, the ratio is highly dependent on the activity, which means context-conditional gating is archived.\relax }{figure.caption.146}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.5}Computation-Performance Tradeoff}{79}{subsection.148}}
\newlabel{ch5:subsec:4-5}{{6.4.5}{79}{Computation-Performance Tradeoff}{subsection.148}{}}
\citation{wang2018non}
\citation{wang2018non}
\citation{korbar2019scsampler}
\@writefile{lot}{\contentsline {table}{\numberline {20}{\ignorespaces Breakdown of the computational cost of TimeGate \textit  {v.s.} SCSampler. Three choices of HeavyNet: ResNet2D (R2D), ShuffleNet3D (S3D) and I3D. The computational cost of LightNet and the gating module is marginal compared to that of the HeavyNet. TimeGate reduces the cost by almost half. Our selector improves over SCSampler.\relax }}{80}{table.caption.147}}
\newlabel{ch5:tbl:4-3}{{20}{80}{Breakdown of the computational cost of TimeGate \textit {v.s.} SCSampler. Three choices of HeavyNet: ResNet2D (R2D), ShuffleNet3D (S3D) and I3D. The computational cost of LightNet and the gating module is marginal compared to that of the HeavyNet. TimeGate reduces the cost by almost half. Our selector improves over SCSampler.\relax }{table.caption.147}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces TimeGate (\textbf  {TG}) is better than SCSampler (\textbf  {SCS}) in reducing computational cost of I3D.\relax }}{80}{figure.caption.149}}
\newlabel{ch5:fig:4-3}{{35}{80}{TimeGate (\textbf {TG}) is better than SCSampler (\textbf {SCS}) in reducing computational cost of I3D.\relax }{figure.caption.149}{}}
\@writefile{lot}{\contentsline {table}{\numberline {21}{\ignorespaces TimeGate improves the performance of the backbone CNNs (\textit  {i.e.} I3D) on the challenging task of multi-label classification of Charades.\relax }}{80}{table.caption.150}}
\newlabel{ch5:tbl:4-5}{{21}{80}{TimeGate improves the performance of the backbone CNNs (\textit {i.e.} I3D) on the challenging task of multi-label classification of Charades.\relax }{table.caption.150}{}}
\citation{hussein2019timeception}
\citation{hussein2019timeception}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces TimeGate improves the performance of the off-the-shelf I3D for recognizing the actions of Charades.\relax }}{81}{figure.caption.152}}
\newlabel{ch5:fig:4-4}{{36}{81}{TimeGate improves the performance of the off-the-shelf I3D for recognizing the actions of Charades.\relax }{figure.caption.152}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.6}Experiments on Charades}{81}{subsection.151}}
\newlabel{subsec:4-6}{{6.4.6}{81}{Experiments on Charades}{subsection.151}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.7}Experiments on MultiThumos}{81}{subsection.153}}
\newlabel{ch5:subsec:4-7}{{6.4.7}{81}{Experiments on MultiThumos}{subsection.153}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.8}Qualitative Results}{81}{subsection.155}}
\newlabel{ch5:subsec:4-8}{{6.4.8}{81}{Qualitative Results}{subsection.155}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22}{\ignorespaces TimeGate improves the performance of I3D when classifying the long-range activities of MultiThmos. Also, it outperforms SCSampler.\relax }}{82}{table.caption.154}}
\newlabel{ch5:tbl:4-9}{{22}{82}{TimeGate improves the performance of I3D when classifying the long-range activities of MultiThmos. Also, it outperforms SCSampler.\relax }{table.caption.154}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Top, frames corresponding to the selected timesteps by TimGate. Bottom, are those discarded by TimeGate. The shown figures are for three activities: ``making sandwich", ``preparing coffee", and ``making pancake". The general observation is that TimeGate tends to discard the segments with little discriminative visual evidences.\relax }}{82}{figure.caption.156}}
\newlabel{ch5:fig:4-9}{{37}{82}{Top, frames corresponding to the selected timesteps by TimGate. Bottom, are those discarded by TimeGate. The shown figures are for three activities: ``making sandwich", ``preparing coffee", and ``making pancake". The general observation is that TimeGate tends to discard the segments with little discriminative visual evidences.\relax }{figure.caption.156}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Distribution of the gating values across time for each activity of Breakfast. In simple activities, such as ``making coffee", most of the selected segments happen in the middle of the video. This means the middle of the video is much more relevant than the other parts. While in complex activities, such as ``making sandwich", the selected segments tend to distribute across the entire video. This means that almost the entire video contains relevant and important segments.\relax }}{82}{figure.caption.157}}
\newlabel{ch5:fig:4-7}{{38}{82}{Distribution of the gating values across time for each activity of Breakfast. In simple activities, such as ``making coffee", most of the selected segments happen in the middle of the video. This means the middle of the video is much more relevant than the other parts. While in complex activities, such as ``making sandwich", the selected segments tend to distribute across the entire video. This means that almost the entire video contains relevant and important segments.\relax }{figure.caption.157}{}}
\citation{hussein2019timeception}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Conclusion}{83}{section.158}}
\newlabel{ch5:sec:conclusions}{{6.5}{83}{Conclusion}{section.158}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}\spacedlowsmallcaps  {Conclusions}}{85}{chapter.159}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:conclusion}{{7}{85}{\texorpdfstring {\spacedlowsmallcaps {Conclusions}}{Conclusions}}{chapter.159}{}}
\@writefile{toc}{\vskip 10pt}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Summary}{85}{section.160}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Discussion}{88}{section.161}}
\@writefile{toc}{\contentsline {chapter}{Samenvatting}{91}{chapter*.162}}
\citation{wang2017non}
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{95}{chapter*.163}}
\bibstyle{abbrv}
\bibdata{thesis}
\bibcite{wikihow}{1}
\bibcite{wikipedia}{2}
\bibcite{charades2017algorithms}{3}
\bibcite{timeceptioncode}{4}
\bibcite{tensorflow2015-whitepaper}{5}
\bibcite{abu2016youtube}{6}
\bibcite{agharwal2016tag}{7}
\bibcite{arandjelovic2013all}{8}
\bibcite{bay2006surf}{9}
\bibcite{bejnordi2019batch}{10}
\bibcite{bhardwaj2019efficient}{11}
\bibcite{bilen2017action}{12}
\bibcite{bilen2016dynamic}{13}
\bibcite{blei2003latent}{14}
\bibcite{carreira2017quo}{15}
\bibcite{chang2015semantic}{16}
\bibcite{chang2016dynamic}{17}
\bibcite{changthey}{18}
\bibcite{chen2019you}{19}
\bibcite{chollet2016xception}{20}
\bibcite{chollet2015keras}{21}
\bibcite{chopra2005learning}{22}
\bibcite{cosmin2017spatio}{23}
\bibcite{dalal2005histograms}{24}
\bibcite{damen2018scaling}{25}
\bibcite{deerwester1990indexing}{26}
\bibcite{donahue2015long}{27}
\bibcite{du2018interaction}{28}
\bibcite{duta2017spatio}{29}
\bibcite{elhoseiny2015zero}{30}
\bibcite{feichtenhofer2019slowfast}{31}
\bibcite{feichtenhofer2018have}{32}
\bibcite{feichtenhofer2016convolutional}{33}
\bibcite{fernandoTPAMIrankpooling}{34}
\bibcite{fernando2017rank}{35}
\bibcite{finn2017model}{36}
\bibcite{fleet2006optical}{37}
\bibcite{fukushima1975cognitron}{38}
\bibcite{fukushima1980neocognitron}{39}
\bibcite{gan2016you}{40}
\bibcite{gavves2015activetransferlearning}{41}
\bibcite{gehring2017convolutional}{42}
\bibcite{ghodrati2018video}{43}
\bibcite{girdhar2019video}{44}
\bibcite{girdhar2017attentional}{45}
\bibcite{girdhar2017actionvlad}{46}
\bibcite{gu2017ava}{47}
\bibcite{habibian2014composite}{48}
\bibcite{habibian2014videostory}{49}
\bibcite{habibian2015discovering}{50}
\bibcite{habibian2015videostory}{51}
\bibcite{habibian2017video2vec}{52}
\bibcite{han2015learning}{53}
\bibcite{hara2017can}{54}
\bibcite{hassibi1993optimal}{55}
\bibcite{he2016deep}{56}
\bibcite{he2015deep}{57}
\bibcite{heilbron2015activitynet}{58}
\bibcite{horn1981determining}{59}
\bibcite{howard2019searching}{60}
\bibcite{howard2017mobilenets}{61}
\bibcite{huang2017densely}{62}
\bibcite{hussein2017unified}{63}
\bibcite{hussein2019timeception}{64}
\bibcite{hussein2019videograph}{65}
\bibcite{hussein2020pic}{66}
\bibcite{hussein2020timegate}{67}
\bibcite{idrees2017thumos}{68}
\bibcite{jain2013better}{69}
\bibcite{jang2016categorical}{70}
\bibcite{ji20123d}{71}
\bibcite{ji20133d}{72}
\bibcite{jiang2014easy}{73}
\bibcite{jiang2014zero}{74}
\bibcite{jiang2015bridging}{75}
\bibcite{jiang2015fast}{76}
\bibcite{jiangfcvid}{77}
\bibcite{jiang2017exploiting}{78}
\bibcite{jiang2011consumer}{79}
\bibcite{jing2016discriminative}{80}
\bibcite{karpathy2014large}{81}
\bibcite{kay2017kinetics}{82}
\bibcite{kiros2015skip}{83}
\bibcite{kopuklu2019resource}{84}
\bibcite{korbar2019scsampler}{85}
\bibcite{krizhevsky2014imagenet}{86}
\bibcite{krizhevsky2012imagenet}{87}
\bibcite{kuehne2014language}{88}
\bibcite{kuehne2011hmdb}{89}
\bibcite{kumar2016audio}{90}
\bibcite{lample2019large}{91}
\bibcite{le2014distributed}{92}
\bibcite{lea2017temporal}{93}
\bibcite{lecun1998gradient}{94}
\bibcite{li2016pruning}{95}
\bibcite{li2017concurrent}{96}
\bibcite{li2018videolstm}{97}
\bibcite{louizos2017learning}{98}
\bibcite{lu2016zero}{99}
\bibcite{maaten2008visualizing}{100}
\bibcite{marszalek2009actions}{101}
\bibcite{mazloom2014conceptlets}{102}
\bibcite{mazloom2015tagbook}{103}
\bibcite{mensink2014costa}{104}
\bibcite{mettes2015bag}{105}
\bibcite{miech2017learnable}{106}
\bibcite{mikolov2013exploiting}{107}
\bibcite{mikolov2013distributed}{108}
\bibcite{muda2010voice}{109}
\bibcite{oneata2013action}{110}
\bibcite{over2013trecvid}{111}
\bibcite{over2014trecvid}{112}
\bibcite{pal1992multilayer}{113}
\bibcite{parmar2019stand}{114}
\bibcite{scikit-learn}{115}
\bibcite{peng2016bag}{116}
\bibcite{pini2019m}{117}
\bibcite{rohrbach2015dataset}{118}
\bibcite{rohrbach2016recognizing}{119}
\bibcite{sanchez2013image}{120}
\bibcite{sandler2018mobilenetv2}{121}
\bibcite{santoro2016meta}{122}
\bibcite{schindler2008action}{123}
\bibcite{schuldt2004recognizing}{124}
\bibcite{searle1980minds}{125}
\bibcite{sener2019zero}{126}
\bibcite{sharma2015action}{127}
\bibcite{sigurdsson2017asynchronous}{128}
\bibcite{sigurdsson2016hollywood}{129}
\bibcite{simonyan2013fisher}{130}
\bibcite{simonyan2014two}{131}
\bibcite{simonyan2014very}{132}
\bibcite{soomro2012ucf101}{133}
\bibcite{sutskever2014sequence}{134}
\bibcite{szegedy2017inception}{135}
\bibcite{szegedy2015going}{136}
\bibcite{szegedy2016rethinking}{137}
\bibcite{szelag2004individual}{138}
\bibcite{takahashi2017aenet}{139}
\bibcite{thomee2016yfcc100m}{140}
\bibcite{tong2001support}{141}
\bibcite{tran2014c3d}{142}
\bibcite{tran2015learning}{143}
\bibcite{tran2019video}{144}
\bibcite{tran2018closer}{145}
\bibcite{uijlings2015video}{146}
\bibcite{van2016wavenet}{147}
\bibcite{varol2017long}{148}
\bibcite{vaswani2017attention}{149}
\bibcite{veit2018convolutional}{150}
\bibcite{velivckovic2017graph}{151}
\bibcite{vieira2012stop}{152}
\bibcite{vinyals2016matching}{153}
\bibcite{wang2011action}{154}
\bibcite{wang2013action}{155}
\bibcite{wang2015towards}{156}
\bibcite{wang2016temporal}{157}
\bibcite{wang2018temporal}{158}
\bibcite{wang2018non}{159}
\bibcite{wang2017non}{160}
\bibcite{wang2018videos}{161}
\bibcite{wu2019long}{162}
\bibcite{wu2014zero}{163}
\bibcite{xie2017aggregated}{164}
\bibcite{xie2017rethinking}{165}
\bibcite{xu2015show}{166}
\bibcite{ye2015eventnet}{167}
\bibcite{yeung2015every}{168}
\bibcite{yeung2018every}{169}
\bibcite{yeung2016end}{170}
\bibcite{zhang2017shufflenet}{171}
\bibcite{zhang2018shufflenet}{172}
\bibcite{zhou2017temporal}{173}
\bibcite{zhou2016places}{174}
\bibcite{Zhou2017YouCookIID}{175}
\bibcite{zhou2018towards}{176}
\bibcite{zoph2016neural}{177}
\bibcite{zoph2018learning}{178}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{104}{chapter*.165}}
